from azure.kusto.data import KustoClient, KustoConnectionStringBuilder
import pandas as pd

# Connect to Azure Data Explorer
cluster = "https://<your_cluster>.kusto.windows.net"
database = "<your_database>"
query = "<your_query_here>"  # Replace with your query

kcsb = KustoConnectionStringBuilder.with_az_cli_authentication(cluster)
client = KustoClient(kcsb)

# Execute Query and Load Data
response = client.execute(database, query)
data = response.primary_results[0].to_dataframe()
print(data.head())

from sklearn.preprocessing import MinMaxScaler

# Data Cleaning
data.dropna(inplace=True)  # Remove nulls
data.drop_duplicates(inplace=True)  # Remove duplicates

# Convert datetime and sort
data['timestamp'] = pd.to_datetime(data['timestamp'])
data.sort_values(by='timestamp', inplace=True)

# Feature Engineering
data['value_diff'] = data['value'].diff()  # Example feature
scaler = MinMaxScaler()
data['value_scaled'] = scaler.fit_transform(data[['value']])
data = data.dropna()  # Drop rows with NaNs created by diff()
print(data.head())

from sklearn.model_selection import train_test_split

# Define Features and Labels
X = data[['value_scaled', 'value_diff']]
y = data['fault_label']  # Replace with your actual label column

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from azureml.core import Workspace, Experiment
from azureml.train.automl import AutoMLConfig

# Connect to Azure ML Workspace
ws = Workspace.from_config()
experiment = Experiment(ws, "adx-anomaly-detection")

# Configure AutoML
automl_config = AutoMLConfig(
    task='classification',
    training_data=pd.concat([X_train, y_train], axis=1),
    label_column_name='fault_label',
    primary_metric='accuracy',
    compute_target="local",
    max_concurrent_iterations=4
)

# Submit AutoML Run
run = experiment.submit(automl_config)
run.wait_for_completion()

# Get the Best Model
best_run, fitted_model = run.get_output()
print(f"Best Model: {best_run}")

from sklearn.metrics import accuracy_score

# Predictions
y_pred = fitted_model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")

# Compare with Abacus API (Example)
import requests
import json

# Mock Abacus API Response
abacus_response = requests.post(
    "<abacus_api_url>",
    headers={"Authorization": "Bearer <api_key>"},
    json={"data": X_test.to_dict(orient="records")}
)

abacus_results = abacus_response.json()
# Assume abacus_results['predictions'] is a list of predictions
print(f"Abacus Model Accuracy: {accuracy_score(y_test, abacus_results['predictions'])}")

from azureml.core.model import InferenceConfig, Model
from azureml.core.webservice import AciWebservice

# Register Model
model = Model.register(workspace=ws, model_path="outputs/model.pkl", model_name="adx-anomaly-model")

# Create Inference Configuration
inference_config = InferenceConfig(entry_script="score.py", environment=fitted_model.environment)

# Deploy the Model
aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)
service = Model.deploy(ws, "anomaly-detection-service", [model], inference_config, aci_config)
service.wait_for_deployment()
print(f"Service State: {service.state}")

# Prepare Data for Inference
new_data = X_test.head(1).to_dict(orient="records")

# Send Request
response = requests.post(
    service.scoring_uri,
    headers={"Content-Type": "application/json"},
    json={"data": new_data}
)
print(f"Prediction: {response.json()}")

import joblib
import json
import numpy as np

def init():
    global model
    model = joblib.load("model.pkl")

def run(data):
    inputs = np.array(data["data"])
    predictions = model.predict(inputs)
    return {"predictions": predictions.tolist()}


adx-anomaly-detection/
├── data/
│   └── sample_data.csv          # Placeholder for any sample data files
├── notebooks/
│   └── exploratory_analysis.ipynb  # Optional Jupyter notebook for analysis
├── src/
│   ├── __init__.py
│   ├── data_extraction.py       # Code for pulling data from ADX
│   ├── data_preprocessing.py    # Code for cleaning and feature engineering
│   ├── model_training.py        # AutoML configuration and model training
│   ├── model_testing.py         # Model evaluation and Abacus API comparison
│   ├── deployment.py            # Model registration and deployment
│   ├── scoring/                 # Scoring folder for endpoint scripts
│   │   └── score.py             # Scoring script for inference
├── requirements.txt             # Required Python packages
├── README.md                    # Overview and instructions
└── .gitignore                   # Files to ignore in version control